import asyncio
import json
import os
import random
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse

import httpx
from bs4 import BeautifulSoup
from config import CONCURRENCY_LIMIT, DELAY_BETWEEN_PAGES, DELAY_BETWEEN_CATEGORIES


# --- –ù–û–í–´–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ---
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —Å–∞–π—Ç—É
CONCURRENCY_LIMIT = 5 
# –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
DELAY_BETWEEN_PAGES = (1.0, 2.5) 
# –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–∞—Ä—Å–∏–Ω–≥–æ–º —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
DELAY_BETWEEN_CATEGORIES = (2.0, 4.0)

class GosAptekaParser:
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Å–∞–π—Ç–∞ gosapteka18.ru.
    –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –æ—Ç–ø—Ä–∞–≤–∫—É HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏.
    """
    def __init__(self, session: httpx.AsyncClient):
        self.base_url = 'https://gosapteka18.ru'
        self.session = session

    async def fetch_html(self, url: str, timeout: int = 20) -> str | None:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç GET-–∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            response = await self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except httpx.RequestError as e:
            print(f"üö´ –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)}")
            return None
        except httpx.HTTPStatusError as e:
            print(f"üö´ –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ {e.response.status_code} –¥–ª—è {url}: {str(e)}")
            return None

class CategoryStructureParser(GosAptekaParser):
    """–ö–ª–∞—Å—Å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å —Å–∞–π—Ç–∞."""
    async def parse_structure(self) -> dict:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∫–∞—Ç–∞–ª–æ–≥–∞.
        """
        print("‚ñ∂Ô∏è –®–∞–≥ 1: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        main_page_url = self.base_url + '/'
        html = await self.fetch_html(main_page_url)
        if not html:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É'}

        try:
            soup = BeautifulSoup(html, 'html.parser')
            catalog_container = soup.find('div', class_='menu-catalog')
            if not catalog_container:
                return {'error': "–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞ 'menu-catalog' –Ω–µ –Ω–∞–π–¥–µ–Ω"}

            structured_categories = {}
            columns = catalog_container.find_all('div', class_='menu-catalog__list')
            if not columns:
                return {'error': "–ö–æ–ª–æ–Ω–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π 'menu-catalog__list' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}

            for col in columns:
                items = col.find_all('div', class_='menu-catalog__item', recursive=False)
                for item in items:
                    parent_link = item.find('a', class_='menu-catalog__link')
                    if not parent_link or not parent_link.text.strip():
                        continue

                    parent_name = parent_link.text.strip()
                    parent_url = self.base_url + parent_link.get('href', '')
                    subcategories_l1 = []
                    submenu_l1 = item.find('div', class_='menu-catalog__sub-menu')
                    
                    if submenu_l1:
                        subitems_l1 = submenu_l1.find_all('div', class_='menu-catalog__sub-item')
                        for subitem in subitems_l1:
                            sub_link = subitem.find('a', class_='menu-catalog__sub-link')
                            if not sub_link: continue
                            
                            sub_name = sub_link.text.strip()
                            sub_url = self.base_url + sub_link.get('href', '')
                            subcategories_l2 = []
                            submenu_l2 = subitem.find('div', class_='menu-catalog__sub2-menu')
                            
                            if submenu_l2:
                                for sub2_link in submenu_l2.find_all('a', class_='menu-catalog__sub2-link'):
                                    subcategories_l2.append({
                                        'name': sub2_link.text.strip(),
                                        'url': self.base_url + sub2_link.get('href', '')
                                    })
                            
                            subcategories_l1.append({
                                'name': sub_name, 'url': sub_url, 'subcategories': subcategories_l2
                            })
                    
                    structured_categories[parent_name] = {'url': parent_url, 'subcategories': subcategories_l1}
            
            print("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–∞.")
            return structured_categories
        except Exception as e:
            print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            return {"error": str(e)}

class ProductLinkParser(GosAptekaParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É—á–µ—Ç–æ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
    """
    def __init__(self, session: httpx.AsyncClient):
        super().__init__(session)
        self.parsed_links = set()
        self.first_page_content = None

    def extract_links_from_page(self, html: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        if not html: return []
        
        soup = BeautifulSoup(html, 'html.parser')
        products_grid = soup.find('div', class_='products__grid')
        if not products_grid: return []
        
        links = products_grid.select('a.product-mini__title-link, a.product-mini__picture')
        
        return list(set([urljoin(self.base_url, link.get('href')) for link in links if link.get('href')]))

    def find_next_page(self, html: str, current_url: str, current_page: int) -> str | None:
        """–ù–∞—Ö–æ–¥–∏—Ç URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É—è 3 –º–µ—Ç–æ–¥–∞."""
        soup = BeautifulSoup(html, 'html.parser')
        
        next_btn = soup.find('a', class_='modern-page-next')
        if next_btn and next_btn.get('href'):
            return urljoin(self.base_url, next_btn.get('href'))
        
        page_links = soup.select('a.pagination__item, a.bx-pagination-container a')
        for link in page_links:
            try:
                if int(link.text.strip()) == current_page + 1:
                    return urljoin(self.base_url, link.get('href'))
            except (ValueError, TypeError):
                continue
        
        parsed_url = urlparse(current_url)
        query_params = parse_qs(parsed_url.query)
        query_params['PAGEN_1'] = [str(current_page + 1)]
        new_query = urlencode(query_params, doseq=True)
        return urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment))

    async def parse_products_from_category(self, start_url: str):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã."""
        print(f"\nüöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {start_url}")
        
        current_url = start_url
        page_num = 1
        all_links_for_category = []
        
        while current_url:
            print(f"üìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ #{page_num}: {current_url}")
            
            html = await self.fetch_html(current_url)
            if not html:
                print("–ü—Ä–µ—Ä—ã–≤–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.")
                break

            if page_num == 1:
                self.first_page_content = html

            page_links = self.extract_links_from_page(html)
            new_links = [link for link in page_links if link not in self.parsed_links]
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(page_links)}, –∏–∑ –Ω–∏—Ö –Ω–æ–≤—ã—Ö: {len(new_links)}")
            
            if not page_links or (page_num > 1 and not new_links) or (page_num > 1 and html == self.first_page_content):
                if not page_links or (page_num > 1 and not new_links):
                    print("‚õî –ù–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—É—Å—Ç–∞, –∑–∞–≤–µ—Ä—à–∞—é.")
                else:
                    print("‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (–∫–æ–Ω—Ç–µ–Ω—Ç –¥—É–±–ª–∏—Ä—É–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É).")
                break

            all_links_for_category.extend(new_links)
            self.parsed_links.update(new_links)
            
            next_url = self.find_next_page(html, current_url, page_num)
            
            if not next_url or next_url == current_url:
                print("‚û°Ô∏è  –°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–≤–µ—Ä—à–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
                break
            
            current_url = next_url
            page_num += 1
            delay = random.uniform(*DELAY_BETWEEN_PAGES)
            await asyncio.sleep(delay)
        
        if all_links_for_category:
            self.save_results(all_links_for_category, start_url)
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(all_links_for_category)}")
        return all_links_for_category

    # –í –∫–ª–∞—Å—Å–µ ProductLinkParser
    def save_results(self, links: list, base_url: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫–∏ –∏ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ JSON-—Ñ–∞–π–ª."""
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ URL
        category_name = urlparse(base_url).path.strip('/').split('/')[-1]

        # –ü–æ–ª—É—á–∞–µ–º "—á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ" –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —Ä–∞–Ω–µ–µ —Å–æ–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        # (–≠—Ç–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π —à–∞–≥, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–∏–º —Ç–∞–∫, –Ω–æ –≤ –∏–¥–µ–∞–ª–µ - –∏—Å–∫–∞—Ç—å –ø–æ URL –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ)

        os.makedirs('category_products', exist_ok=True)
        filepath = os.path.join('category_products', f"{category_name}.json")

        output_data = {
            'category_url': base_url,
            'category_name_slug': category_name, # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∏–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            'product_urls': sorted(list(set(links)))
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {filepath}")
        
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'
    }
    
    # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤—Å–µ–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏
    async with httpx.AsyncClient(headers=headers, follow_redirects=True) as session:
        structure_parser = CategoryStructureParser(session)
        catalog_data = await structure_parser.parse_structure()

        if 'error' in catalog_data or not catalog_data:
            print(f"üö´ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {catalog_data.get('error', '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π.')}")
            return
        
        with open('categories_structure.json', 'w', encoding='utf-8') as f:
            json.dump(catalog_data, f, ensure_ascii=False, indent=4)
        print("\nüíæ –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'categories_structure.json'\n" + "="*50)

        product_parser = ProductLinkParser(session)
        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
        tasks = []

        async def worker(cat_url, sub_cat_name):
            """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞—Ñ–æ—Ä–∞."""
            async with semaphore:
                print(f"‚ú® –ó–∞–ø—É—Å–∫–∞—é –∑–∞–¥–∞—á—É –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: '{sub_cat_name}'")
                await product_parser.parse_products_from_category(cat_url)
                delay = random.uniform(*DELAY_BETWEEN_CATEGORIES)
                print(f"--- –ü–∞—É–∑–∞ {delay:.2f} —Å–µ–∫. –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π ---")
                await asyncio.sleep(delay)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ URL –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫ –∏ —Å–æ–∑–¥–∞–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á—É
        for parent_name, parent_data in catalog_data.items():
            if not parent_data.get('subcategories'):
                print(f"INFO: –í '{parent_name}' –Ω–µ—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
                continue
                
            for subcategory in parent_data['subcategories']:
                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                task = asyncio.create_task(worker(subcategory['url'], subcategory['name']))
                tasks.append(task)

        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        await asyncio.gather(*tasks)

    print("\n\nüéâ –í—Å–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã! –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω.")

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω—É—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    asyncio.run(main())