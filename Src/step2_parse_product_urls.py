import requests
import json
from bs4 import BeautifulSoup
import time
from pathlib import Path
import logging
import re
from urllib.parse import urljoin

# –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–º–ø–æ—Ä—Ç –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª '1_parse_categories.py' –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ.
try:
    from step1_parse_categories import GosAptekaParser
except ImportError:
    print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å 'GosAptekaParser'. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª '1_parse_categories.py' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
    exit()


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å –∏ –≤ —Ñ–∞–π–ª
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('url_parser.log', encoding='utf-8', mode='w'), # 'w' –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ –ª–æ–≥–∞ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class UrlCollector(GosAptekaParser):
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ URL-–∞–¥—Ä–µ—Å–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è.
    """
    def __init__(self):
        super().__init__()
        self.processed_urls = set()  # –ö—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL —Ç–æ–≤–∞—Ä–æ–≤, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–µ–π
        self.failed_urls = set()     # URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        self.session.headers.update({
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })

    def get_first_level_subcategory_urls(self, categories_data: dict) -> list:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç URL-–∞–¥—Ä–µ—Å–∞ –¢–û–õ–¨–ö–û –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
        –ï—Å–ª–∏ —É —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—ë URL.
        """
        urls = set()
        logger.info("‚ñ∂Ô∏è –®–∞–≥ 2.1: –°–±–æ—Ä URL –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è...")
        
        for parent_name, parent_data in categories_data.items():
            subcategories_l1 = parent_data.get('subcategories', [])
            
            if not subcategories_l1:
                # –ï—Å–ª–∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ—Ç, –±–µ—Ä–µ–º URL —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π
                if parent_data.get('url'):
                    logger.info(f"  - –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{parent_name}' –±–µ–∑ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–µ –æ—Å–Ω–æ–≤–Ω–∞—è —Å—Å—ã–ª–∫–∞.")
                    urls.add(parent_data['url'])
            else:
                # –ò–Ω–∞—á–µ –±–µ—Ä–µ–º –≤—Å–µ URL –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
                for sub_cat in subcategories_l1:
                    if sub_cat.get('url'):
                        urls.add(sub_cat['url'])

        unique_urls = sorted(list(urls))
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
        return unique_urls

    def extract_products_from_page(self, soup: BeautifulSoup, page_url: str) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤."""
        product_urls = []
        
        # –°–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
        product_selectors = [
            'div.product-mini-card__container a.product-mini-card__name',
            'a.product-card__title'
        ]
        
        found_links = []
        for selector in product_selectors:
            found_links = soup.select(selector)
            if found_links:
                break # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —Å—Ä–∞–±–æ—Ç–∞–≤—à–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä
        
        if not found_links:
            logger.warning("  ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã.")
            return []

        for link in found_links:
            href = link.get('href')
            if href:
                # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –≤ –∞–±—Å–æ–ª—é—Ç–Ω—É—é
                full_url = urljoin(self.base_url, href)
                if full_url not in self.processed_urls:
                    product_urls.append(full_url)
                    self.processed_urls.add(full_url)
                    
        return product_urls

    def find_next_page(self, soup: BeautifulSoup, current_url: str) -> str or None:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏."""
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç 'a' —Å –∫–ª–∞—Å—Å–æ–º 'pagination__item' –∏ '_next', –Ω–æ –±–µ–∑ –∫–ª–∞—Å—Å–∞ '_disabled'
        next_page_tag = soup.select_one('a.pagination__item._next:not(._disabled)')
        
        if next_page_tag and next_page_tag.get('href'):
            next_page_url = urljoin(current_url, next_page_tag['href'])
            return next_page_url
        return None

    def parse_product_urls_from_category(self, category_url: str, max_pages: int = 50) -> list:
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ URL —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –≤–∫–ª—é—á–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—é."""
        all_products_in_category = []
        current_url = category_url
        page_count = 0
        
        logger.info(f"  - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
        
        while current_url and page_count < max_pages:
            page_count += 1
            logger.info(f"    - –°–∫–∞–Ω–∏—Ä—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_count}: {current_url}")
            
            html = self.fetch_html(current_url)
            if not html:
                logger.error(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É. –ü—Ä–æ–ø—É—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
                self.failed_urls.add(category_url)
                break
            
            soup = BeautifulSoup(html, 'html.parser')
            
            page_products = self.extract_products_from_page(soup, current_url)
            if page_products:
                all_products_in_category.extend(page_products)
                logger.info(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_products)} —Å—Å—ã–ª–æ–∫.")
            
            next_url = self.find_next_page(soup, current_url)
            if next_url:
                current_url = next_url
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
            else:
                logger.info("    üèÅ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
        
        logger.info(f"  - –ò—Ç–æ–≥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –Ω–∞–π–¥–µ–Ω–æ {len(all_products_in_category)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã.")
        return all_products_in_category


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞."""
    collector = UrlCollector()
    logger.info("‚ñ∂Ô∏è –®–∞–≥ 2: –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã...")
    
    # --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ ---
    categories_file = Path('categories.json')
    if not categories_file.exists():
        logger.error(f"üö´ –§–∞–π–ª {categories_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ '1_parse_categories.py'.")
        return

    try:
        with open(categories_file, 'r', encoding='utf-8') as f:
            all_categories = json.load(f)
    except json.JSONDecodeError:
        logger.error(f"üö´ –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {categories_file}. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")
        return

    if not all_categories or 'error' in all_categories:
        logger.error("üö´ –§–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—É—Å—Ç –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫—É. –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return

    # --- 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ---
    category_urls_to_parse = collector.get_first_level_subcategory_urls(all_categories)
    
    if not category_urls_to_parse:
        logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ URL –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    # --- 3. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞ ---
    all_product_urls = set()
    total_categories = len(category_urls_to_parse)

    start_time = time.time()
    
    for i, cat_url in enumerate(category_urls_to_parse, 1):
        logger.info(f"\n--- –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total_categories} ---")
        try:
            product_links = collector.parse_product_urls_from_category(cat_url)
            if product_links:
                all_product_urls.update(product_links)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫—Ä—É–ø–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            time.sleep(1.5) 
            
        except KeyboardInterrupt:
            logger.warning("\nüõë –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            break
        except Exception as e:
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {cat_url}: {e}")
            collector.failed_urls.add(cat_url)
            continue
    
    end_time = time.time()
    logger.info(f"\n--- ‚è∞ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥ ---")

    # --- 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    if not all_product_urls:
        logger.error("üö´ –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã.")
        return

    output_file = Path('product_urls_l1.json')
    sorted_urls = sorted(list(all_product_urls))
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(sorted_urls, f, ensure_ascii=False, indent=4)
            
        logger.info(f"‚úÖ –£–°–ü–ï–•! –°–æ–±—Ä–∞–Ω–æ {len(sorted_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã.")
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: '{output_file}'")

        if collector.failed_urls:
            failed_file = Path('failed_categories.log')
            logger.warning(f"‚ö†Ô∏è {len(collector.failed_urls)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å.")
            with open(failed_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(sorted(list(collector.failed_urls))))
            logger.warning(f"üóÇÔ∏è –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{failed_file}'")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª: {e}")


if __name__ == "__main__":
    main()