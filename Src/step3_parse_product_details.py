# 3_parse_product_details.py
import requests
import json
from bs4 import BeautifulSoup
import time
import os

# –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–º–ø–æ—Ä—Ç –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞
from step1_parse_categories import GosAptekaParser

class ProductScraper(GosAptekaParser):
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Ç–æ–≤–∞—Ä–µ.
    –ù–∞—Å–ª–µ–¥—É–µ—Ç –º–µ—Ç–æ–¥—ã GosAptekaParser.
    """
    def parse_product_details(self, product_url):
        """–°–æ–±–∏—Ä–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–∞."""
        html = self.fetch_html(product_url)
        if not html: 
            return None

        soup = BeautifulSoup(html, 'html.parser')
        details = {'url': product_url, 'name': 'N/A', 'price': 'N/A', 'manufacturer': 'N/A', 'description': 'N/A'}
        
        try:
            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
            name_tag = soup.select_one('h1.product-title')
            if name_tag:
                details['name'] = name_tag.text.strip()
            
            # –¶–µ–Ω–∞
            price_tag = soup.select_one('div.product-price__value')
            if price_tag:
                price_text = ''.join(c for c in price_tag.text if c.isdigit() or c in '.,')
                details['price'] = float(price_text.replace(',', '.'))
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å
            # –ò—â–µ–º –±–ª–æ–∫ "–û —Ç–æ–≤–∞—Ä–µ" –∏ –≤ –Ω–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –±—Ä–µ–Ω–¥
            about_section = soup.select_one('div.product-about')
            if about_section:
                manufacturer_tag = about_section.find('a', class_='product-about__brand-link')
                if manufacturer_tag:
                    details['manufacturer'] = manufacturer_tag.text.strip()

            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc_div = soup.select_one('div.product-description__text')
            if desc_div:
                details['description'] = desc_div.text.strip()

        except Exception as e:
            print(f"‚ùóÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑–æ–±—Ä–∞—Ç—å {product_url}. –û—à–∏–±–∫–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª–∏ —Å–æ–±—Ä–∞—Ç—å
            return details
        
        return details

def main():
    scraper = ProductScraper()
    print("\n‚ñ∂Ô∏è –®–∞–≥ 3: –°–±–æ—Ä –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–∞—Ö...")

    urls_file = 'product_urls.json'
    if not os.path.exists(urls_file):
        print(f"üö´ –§–∞–π–ª {urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ '2_parse_product_urls.py'")
        return

    with open(urls_file, 'r', encoding='utf-8') as f:
        product_urls = json.load(f)
    
    total_urls = len(product_urls)
    if total_urls == 0:
        print("‚ÑπÔ∏è –§–∞–π–ª 'product_urls.json' –ø—É—Å—Ç. –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–±–æ—Ä–∞.")
        return
        
    print(f"‚è≥ –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {total_urls} —Ç–æ–≤–∞—Ä–æ–≤...")
    
    all_products_data = []
    for i, url in enumerate(product_urls, 1):
        print(f"  üì¶ –¢–æ–≤–∞—Ä {i}/{total_urls}: {url}")
        data = scraper.parse_product_details(url)
        if data:
            all_products_data.append(data)
        time.sleep(0.5) # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {len(all_products_data)} —Ç–æ–≤–∞—Ä–∞—Ö.")

    with open('products_data.json', 'w', encoding='utf-8') as f:
        json.dump(all_products_data, f, ensure_ascii=False, indent=4)

    print("üíæ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'products_data.json'")

if __name__ == "__main__":
    main()