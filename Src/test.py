import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class SimpleProductParser:
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
    """
    def __init__(self):
        # –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω—ã—Ö —Å—Å—ã–ªs–æ–∫ 
        self.base_url = 'https://gosapteka18.ru'
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏, —á—Ç–æ–±—ã —Å–∞–π—Ç –¥—É–º–∞–ª, —á—Ç–æ –º—ã –±—Ä–∞—É–∑–µ—Ä
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def fetch_html(self, url: str) -> str | None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            response = self.session.get(url, timeout=15)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ (–∫–æ–¥ 200)
            response.raise_for_status() 
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"üö´ –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None

    def parse_and_print_products(self, page_url: str):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: –ø–∞—Ä—Å–∏—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã."""
        print(f"‚ñ∂Ô∏è –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_url}")
        
        html = self.fetch_html(page_url)
        if not html:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML. –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É.")
            return

        soup = BeautifulSoup(html, 'html.parser')

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä—É –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞
        # –≠—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞
        product_links = soup.select('a.product-card__title')
        
        if not product_links:
            print("‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Å–µ–ª–µ–∫—Ç–æ—Ä—É.")
            return

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(product_links)}. –í—ã–≤–æ–∂—É —Å—Å—ã–ª–∫–∏:")
        
        # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ <a>
        for link in product_links:
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ href (—Å–∞–º—É —Å—Å—ã–ª–∫—É)
            href = link.get('href')
            if href:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, /product/123)
                # –≤ –∞–±—Å–æ–ª—é—Ç–Ω—É—é (https://gosapteka18.ru/product/123)
                full_url = urljoin(self.base_url, href)
                print(full_url)


# --- –û–°–ù–û–í–ù–ê–Ø –ß–ê–°–¢–¨ ---
if __name__ == "__main__":
    
    target_url = "https://gosapteka18.ru/catalog/lekarstvennye-preparaty-i-bady/bolezni-sustavov/"

    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –Ω–∞—à–µ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
    parser = SimpleProductParser()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏
    parser.parse_and_print_products(target_url)