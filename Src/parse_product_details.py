import asyncio
import os
import re
from urllib.parse import urljoin

import asyncpg
import httpx
from bs4 import BeautifulSoup, Tag

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –≥–¥–µ –ª–µ–∂–∞—Ç .txt —Ñ–∞–π–ª—ã —Å URL —Ç–æ–≤–∞—Ä–æ–≤
URLS_DIR = 'category_products'
# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—á–∏—â–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
IMAGES_DIR = 'static/images/products'
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î (–∏–∑ docker-compose.yml)
DB_CONFIG = {
    'user': 'user_farm',
    'password': 'password_farm',
    'database': 'db_farm',
    'host': 'localhost'
}
# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É
CONCURRENCY_LIMIT = 10


class ProductProcessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞: –ø–∞—Ä—Å–∏–Ω–≥, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î.
    """
    def __init__(self, session: httpx.AsyncClient, db_pool: asyncpg.Pool):
        self.base_url = 'https://gosapteka18.ru'
        self.session = session
        self.db_pool = db_pool

    async def fetch_html(self, url: str) -> str | None:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            # –£–º–µ—Ä–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
            await asyncio.sleep(0.5)
            response = await self.session.get(url, timeout=20)
            response.raise_for_status()
            return response.text
        except httpx.RequestError as e:
            print(f"üö´ –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

    async def process_product(self, product_url: str):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–∞ –ë–ï–ó —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        print(f"‚è≥ –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è: {product_url}")
        html = await self.fetch_html(product_url)
        if not html:
            return

        soup = BeautifulSoup(html, 'html.parser')
        
        title = self._get_title(soup)
        if not title:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è {product_url}, –ø—Ä–æ–ø—É—Å–∫.")
            return

        description_dict = self._get_description(soup)
        description_text = "\n\n".join([f"{k}:\n{v}" for k, v in description_dict.items()])
        
        # –ü—Ä–æ—Å—Ç–æ –ø–æ–ª—É—á–∞–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –Ω–µ —Å–∫–∞—á–∏–≤–∞–µ–º –µ–≥–æ
        original_image_url = self._get_image(soup)

        # –õ–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –æ—Å—Ç–∞–µ—Ç—Å—è
        medicine_type_name = "–û–±—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö, –ø–µ—Ä–µ–¥–∞–≤–∞—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL
        await self.save_to_db({
            'name': title,
            'description': description_text,
            'image_url': original_image_url, # <-- –í–ê–ñ–ù–û: –∑–¥–µ—Å—å —Ç–µ–ø–µ—Ä—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL
            'type_name': medicine_type_name
        })
        
    async def save_to_db(self, data: dict):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        –ü–æ–ª–µ image_url —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫.
        """
        # ... (–∫–æ–¥ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô, —Ç.–∫. –æ–Ω –ø—Ä–æ—Å—Ç–æ –ø–∏—à–µ—Ç —Ç–æ, —á—Ç–æ –µ–º—É –ø–µ—Ä–µ–¥–∞–ª–∏)
        # –û–Ω –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–∏—à–µ—Ç https://... —Å—Å—ã–ª–∫—É –≤ –ø–æ–ª–µ image_url.
        async with self.db_pool.acquire() as connection:
            async with connection.transaction():
                type_id = await connection.fetchval(
                    """
                    INSERT INTO medicine_types (name) VALUES ($1)
                    ON CONFLICT (name) DO NOTHING;
                    
                    SELECT id FROM medicine_types WHERE name = $1;
                    """,
                    data['type_name']
                )

                await connection.execute(
                    """
                    INSERT INTO medicines (name, description, image_url, type_id)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (name) DO UPDATE SET
                        description = EXCLUDED.description,
                        image_url = EXCLUDED.image_url,
                        type_id = EXCLUDED.type_id;
                    """,
                    data['name'], data['description'], data['image_url'], type_id
                )
        print(f"üíæ –î–∞–Ω–Ω—ã–µ (—Å URL –∫–∞—Ä—Ç–∏–Ω–∫–∏) –¥–ª—è —Ç–æ–≤–∞—Ä–∞ '{data['name']}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î.")

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ (–≤–∑—è—Ç—ã –∏–∑ –≤–∞—à–µ–≥–æ test.py –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã)
    def _get_title(self, soup: BeautifulSoup) -> str:
        title_tag = soup.select_one('h1.title.headline-main__title.product-card__title')
        return title_tag.get_text(strip=True) if title_tag else ''

    def _get_image(self, soup: BeautifulSoup) -> str:
        image_tag = soup.select_one('img.product-card__picture-view-img')
        if image_tag and 'src' in image_tag.attrs:
            return urljoin(self.base_url, image_tag['src'])
        return ''

    def _get_description(self, soup: BeautifulSoup) -> dict:
        description_block = soup.select_one('div.product-card__description')
        if not description_block: return {}
        
        sections = {}
        for header in description_block.find_all('h4'):
            header_text = header.get_text(strip=True)
            content = []
            for sibling in header.find_next_siblings():
                if sibling.name == 'h4':
                    break
                if isinstance(sibling, Tag):
                    content.append(sibling.get_text(" ", strip=True))
            sections[header_text] = " ".join(content)
        return sections

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —á–∏—Ç–∞–µ—Ç URL –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏."""
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –∏–∑ –≤—Å–µ—Ö .txt —Ñ–∞–π–ª–æ–≤
    all_urls = set()
    if not os.path.exists(URLS_DIR):
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è '{URLS_DIR}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–µ—Ä–≤—ã–π –ø–∞—Ä—Å–µ—Ä.")
        return

    for filename in os.listdir(URLS_DIR):
        if filename.endswith('.txt'):
            with open(os.path.join(URLS_DIR, filename), 'r', encoding='utf-8') as f:
                all_urls.update(line.strip() for line in f)
    
    if not all_urls:
        print("ü§∑ –ù–µ –Ω–∞–π–¥–µ–Ω–æ URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

    # –°–æ–∑–¥–∞–µ–º –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ë–î
    db_pool = await asyncpg.create_pool(**DB_CONFIG)
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
    
    async def worker(url, processor):
        async with semaphore:
            await processor.process_product(url)

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    async with httpx.AsyncClient(headers=headers, follow_redirects=True) as session:
        processor = ProductProcessor(session, db_pool)
        tasks = [asyncio.create_task(worker(url, processor)) for url in all_urls]
        await asyncio.gather(*tasks)

    await db_pool.close()
    print("\n\nüéâ –í—Å–µ —Ç–æ–≤–∞—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")

if __name__ == "__main__":
    asyncio.run(main())